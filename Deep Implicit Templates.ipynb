{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iNIGZOAoCaBI",
    "outputId": "1d928ac3-de11-44bc-8105-e0f03d1d183a"
   },
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "#from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "#drive.mount('/content/drive')# Load the Drive helper and mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E0NOgmO3Cz9B"
   },
   "outputs": [],
   "source": [
    "# data_path = \"data/04256520\"#\"/content/drive/My Drive/Kaist/[2022]CS492/Implementation/04256520\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WUsTzHL80ZxB"
   },
   "outputs": [],
   "source": [
    "# Library Import\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data as data_utils\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "#import sklearn\n",
    "#import pandas as pd\n",
    "import os\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # CHANGED BY OLIVIA\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\" # CHANGED BY OLIVIA\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3YDm8_gSWyl",
    "outputId": "f90de564-7f29-42aa-81d9-fe9d955a479c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of Dataset: 3173\n"
     ]
    }
   ],
   "source": [
    "DIRECTORY_MODELS = 'data/SdfSamples/ShapeNetV2/04256520' #'data/SdfSamples/ShapeNetV2/04256520'\n",
    "MODEL_EXTENSION = '.npz'\n",
    "def get_model_files():\n",
    "    for directory, _, files in os.walk(DIRECTORY_MODELS):\n",
    "        for filename in files:\n",
    "            if filename.endswith(MODEL_EXTENSION):\n",
    "                yield  os.path.join(filename)\n",
    "    \n",
    "def get_model_dir():\n",
    "    for directory, _, files in os.walk(DIRECTORY_MODELS):\n",
    "        for filename in files:\n",
    "            if filename.endswith(MODEL_EXTENSION):\n",
    "                yield os.path.join(directory, filename)\n",
    "                \n",
    "files = list(get_model_files())\n",
    "dirs = list(get_model_dir())\n",
    "\n",
    "print(\"the number of Dataset:\", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAjkHc8kSW1e",
    "outputId": "13e47aa2-d4a2-46aa-d8a5-1280b6406245"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9b1594f6864f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train set!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles_dir_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# /data.../...npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfiles_npz_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# ...npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_npz_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dirs' is not defined"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "num_test = 100\n",
    "# train set!\n",
    "files_dir_train = dirs[:num_train] # /data.../...npz\n",
    "files_npz_train = files[:num_train] # ...npz\n",
    "len(files_npz_train)\n",
    "files_dir_test = dirs[num_train+1:num_train+num_test+1] # /data.../...npz\n",
    "files_npz_test = files[num_train+1:num_train+num_test+1] # ...npz\n",
    "\n",
    "print(\"the number of Dataset(train):\", len(files_npz_train))\n",
    "print(\"the number of Dataset(test):\", len(files_npz_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qiD43TbYSW4K"
   },
   "outputs": [],
   "source": [
    "mesh_directory = 'mesh/'\n",
    "latent_directory = 'latent/'\n",
    "for i, npz in enumerate(files_npz_train):\n",
    "    if \"npz\" not in npz:\n",
    "        continue\n",
    "    mesh_fname = os.path.join(mesh_directory, npz[:-4] + \".ply\") #mesh \n",
    "    latent_fname = os.path.join(latent_directory, npz[:-4] + \".pth\") #latent\n",
    "    #result directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7G8iKd-Sf8w",
    "outputId": "528ad679-1a32-426e-d1eb-61eed358247a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 1000/1000 [00:13<00:00, 71.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# code from the DeepSDF Git\n",
    "# https://github.com/facebookresearch/DeepSDF/blob/48c19b8d49ed5293da4edd7da8c3941444bc5cd7/deep_sdf/data.py\n",
    "\n",
    "\n",
    "def sdf_dispenser(data, subsample=None):\n",
    "    if subsample is None:\n",
    "        return data\n",
    "    pos_tensor = data[0]\n",
    "    neg_tensor = data[1]\n",
    "\n",
    "    # split the sample into half\n",
    "    half = int(subsample / 2)\n",
    "\n",
    "    pos_size = pos_tensor.shape[0]\n",
    "    neg_size = neg_tensor.shape[0]\n",
    "\n",
    "    pos_start_ind = random.randint(0, pos_size - half)\n",
    "    sample_pos = pos_tensor[pos_start_ind : (pos_start_ind + half)]\n",
    "\n",
    "    if neg_size <= half:\n",
    "        random_neg = (torch.rand(half) * neg_tensor.shape[0]).long()\n",
    "        sample_neg = torch.index_select(neg_tensor, 0, random_neg)\n",
    "    else:\n",
    "        neg_start_ind = random.randint(0, neg_size - half)\n",
    "        sample_neg = neg_tensor[neg_start_ind : (neg_start_ind + half)]\n",
    "\n",
    "    samples = torch.cat([sample_neg, sample_pos], 0)\n",
    "    #Random Shuffle!\n",
    "    randidx = torch.randperm(samples.shape[0])\n",
    "    samples = torch.index_select(samples, 0, randidx)\n",
    "\n",
    "    return samples\n",
    "\n",
    "def remove_nans(tensor):\n",
    "    tensor_nan = torch.isnan(tensor[:, 3])\n",
    "    return tensor[~tensor_nan, :]\n",
    "\n",
    "\n",
    "class SDFSamples(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        npyfiles,\n",
    "        subsample,\n",
    "        print_filename=False,\n",
    "        num_files=1000000,\n",
    "    ):\n",
    "        self.subsample = subsample\n",
    "\n",
    "        self.data_source = data_source\n",
    "        self.npyfiles = npyfiles\n",
    "        self.loaded_data = []\n",
    "        for f in tqdm.tqdm(self.npyfiles, ascii=True):\n",
    "            filename = os.path.join(f)\n",
    "            npz = np.load(filename)\n",
    "            pos_tensor = remove_nans(torch.from_numpy(npz[\"pos\"]))\n",
    "            neg_tensor = remove_nans(torch.from_numpy(npz[\"neg\"]))\n",
    "\n",
    "            \n",
    "            #Random Shuffle\n",
    "            self.loaded_data.append(\n",
    "                [\n",
    "                    pos_tensor[torch.randperm(pos_tensor.shape[0])],\n",
    "                    neg_tensor[torch.randperm(neg_tensor.shape[0])],\n",
    "                ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.npyfiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.npyfiles[idx]\n",
    "        return (\n",
    "            sdf_dispenser(self.loaded_data[idx], self.subsample),\n",
    "            idx,\n",
    "        )\n",
    "    \n",
    "data_source = DIRECTORY_MODELS\n",
    "file_dirs = files_dir_train\n",
    "num_samp_per_scene = 5000 #00 #\n",
    "scene_per_batch = 1 #Change!\n",
    "num_data_loader_threads = 1\n",
    "npyfiles = files_dir_train # /data.../...npz\n",
    "\n",
    "sdf_data=SDFSamples(npyfiles,num_samp_per_scene)\n",
    "# Data Load\n",
    "sdf_loader = data_utils.DataLoader(\n",
    "            sdf_data,\n",
    "            batch_size=scene_per_batch,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            drop_last=True, #Don't use the Last Batch\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YzAEbAurSf_F"
   },
   "outputs": [],
   "source": [
    "# Data Load\n",
    "sdf_loader = data_utils.DataLoader(\n",
    "            sdf_data,\n",
    "            batch_size=scene_per_batch,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            drop_last=True,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Q3-dpbnSo0L",
    "outputId": "c5961f85-38c4-4589-ef49-74e1c3256344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "used device cuda\n"
     ]
    }
   ],
   "source": [
    "# use cuda GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"used device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SkvoJcqzHX86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!nvidia-smi\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdcopZ2uGwmq",
    "outputId": "adb18591-36cf-4ee4-fd12-09fae25e6bf4"
   },
   "source": [
    "## Deep Implicit Templates Model\n",
    "### LSTM & MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KojpdAWJSWuz"
   },
   "outputs": [],
   "source": [
    "# From the DIT github\n",
    "# https://github.com/ZhengZerong/DeepImplicitTemplates\n",
    "\n",
    "#weight initialization \n",
    "def weight_initial(self):\n",
    "    for m in self.modules():\n",
    "        for n, p in m.named_parameters():\n",
    "            if 'weight_ih' in n:\n",
    "                nn.init.kaiming_normal_(p.data)\n",
    "            elif 'weight_hh' in n:\n",
    "                nn.init.orthogonal_(p.data)\n",
    "            elif 'bias' in n:\n",
    "                p.data.fill_(0)\n",
    "            \n",
    "#output weight initialization\n",
    "def weights_out_init(self):\n",
    "    for m in self.modules():\n",
    "        for n, p in m.named_parameters():\n",
    "            if 'weight' in n:\n",
    "                nn.init.uniform_(p.data, -1e-5, 1e-5)\n",
    "            elif 'bias' in n:\n",
    "                nn.init.constant_(p.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the DIT github\n",
    "# https://github.com/ZhengZerong/DeepImplicitTemplates\n",
    "\n",
    "class Warper(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            steps,\n",
    "            latent_size,\n",
    "            h_size,\n",
    "            dim,\n",
    "            linee = False\n",
    "    ):\n",
    "        super(Warper, self).__init__()\n",
    "        #latent vector + xyz coordinates\n",
    "        out_len = 6\n",
    "        self.n_feature_channels = latent_size + 3\n",
    "        self.h_size = h_size\n",
    "        self.steps = steps\n",
    "        self.lstm = nn.LSTMCell(input_size=self.n_feature_channels,\n",
    "                                hidden_size=h_size)\n",
    "        lstm_layer = self.lstm\n",
    "        out_len = 6\n",
    "        \n",
    "        #LSTM Cell weight initial\n",
    "        lstm_layer.apply(weight_initial)\n",
    "        for n, p in lstm_layer.named_parameters():\n",
    "            if \"bias\" in n: # continue\n",
    "                si = p.size(0)\n",
    "                start, end = si // 4, si // 2\n",
    "                p.data[start:end].fill_(1.)\n",
    "\n",
    "        self.out_feature = nn.Linear(h_size, out_len)\n",
    "        # Out Layer Weight Initializer\n",
    "        self.out_feature.apply(weights_out_init)\n",
    "\n",
    "    def forward(self, input):\n",
    "        xyz = input[:, -3:] #Change\n",
    "        lat = input[:, :-3] # Fix\n",
    "        states = [None]\n",
    "        warped_xyzs = []\n",
    "        warping_p = []\n",
    "\n",
    "        # Step for 8 times\n",
    "        for s in range(self.steps):\n",
    "            cell_st = self.lstm( torch.cat([lat, xyz], dim=1), states[-1]) #Latest state!\n",
    "            rgd = cell_st[0].requires_grad\n",
    "            if rgd:\n",
    "                cell_st[0].register_hook(lambda x: x.clamp(min=-10, max=10))\n",
    "            a = self.out_feature(cell_st[0])\n",
    "            \n",
    "            int_xyz = torch.addcmul(a[:, 3:], (a[:, :3]+1), xyz)\n",
    "                                    #SDF + 1*(1+ w_xyz)*xyz\n",
    "            xyz_t = int_xyz\n",
    "            if (s+1) % 2 == 0:\n",
    "                warped_xyzs.append(xyz_t)\n",
    "            states.append(cell_st)\n",
    "            warping_p.append(a)\n",
    "\n",
    "                #중간에 있는 값들!\n",
    "            xyz = int_xyz\n",
    "        return xyz, warping_p, warped_xyzs \n",
    "        #latest xyz, parameter, intermediate xyzs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xc9UHpltSo29"
   },
   "outputs": [],
   "source": [
    "# MLP : SDF Decoder\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers, wn_layers, weight_norm, dropout_layers, dropout_prob):\n",
    "        \"\"\"\n",
    "            Initialize the MLP that extracts the template SDF\n",
    "        \"\"\"\n",
    "        \n",
    "        # layers: number of layers + size of each layer\n",
    "        # [256, 256, 256, 256, 256] \n",
    "        # => 5 layers with 256 neurons each\n",
    "        \n",
    "        # wn_layers: layer indices in which normalization is used\n",
    "        # [0, 1, 2, 3, 4]\n",
    "        \n",
    "        # weight_norm: bool\n",
    "        # which normalization to use????? TODO\n",
    "        \n",
    "        # dropout_layers: layer indices in which dropout is used\n",
    "        # [0, 1, 2, 3, 4]\n",
    "        \n",
    "        # dropout_prob: probability for dropout\n",
    "        # 0.05\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.numlayers = len(layers)+1\n",
    "        #print(\"numlayers\", self.numlayers)\n",
    "        \n",
    "        in_dim = 3\n",
    "        out_dim = 1\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(self.numlayers):\n",
    "            #print(i)\n",
    "            # layer input feature count (for first layer: in_dim)\n",
    "            in_features = in_dim if (i == 0) else layers[i-1]\n",
    "            # layer output feature count (for last layer: out_dim)\n",
    "            out_features = out_dim if (i == (len(layers))) else layers[i]\n",
    "            \n",
    "            # fully connected layer\n",
    "            layer = nn.Linear(in_features, out_features)\n",
    "            \n",
    "            modules = [layer]\n",
    "            if i in wn_layers:\n",
    "                # weight normalization layer\n",
    "                if False:  ##########weight_norm: ########## TODO\n",
    "                    layer = nn.utils.weight_norm(layer) # ????????\n",
    "                    modules[0] = layer\n",
    "                else:\n",
    "                    modules.append(nn.LayerNorm(out_features))\n",
    "                    modules.append(nn.BatchNorm1d(out_features)) #ADD!\n",
    "            \n",
    "            # activation (tanh if last layer, else relu)\n",
    "            activation = nn.Tanh() if (i == (len(layers))) else nn.ReLU()\n",
    "            modules.append(activation)\n",
    "            \n",
    "            # dropout\n",
    "            if i in dropout_layers: ### TODO only if training??\n",
    "                modules.append(nn.Dropout(dropout_prob))\n",
    "            \n",
    "            \n",
    "            # if last layer (and tanh) => another tanh??\n",
    "            #if (i == (len(layers))): #### TODO and tanh\n",
    "            #    modules.append(nn.Tanh())\n",
    "            \n",
    "            sequential = nn.Sequential(*modules)\n",
    "            self.layers.append(sequential)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7NsUzlMf0sVi"
   },
   "outputs": [],
   "source": [
    "# output: warped point (from warper) and predicted SDF (from MLP)\n",
    "\n",
    "class DIT(nn.Module):\n",
    "    def __init__(self, mlp_args, lstm_args):\n",
    "        \"\"\"\n",
    "            the entire network consisting of the LSTM and the MLP\n",
    "        \"\"\"\n",
    "        super(DIT, self).__init__()\n",
    "        self.mlp = MLP(**mlp_args)\n",
    "        self.lstm = Warper(**lstm_args)\n",
    "        \n",
    "        #print(\"mlp weigths\", self.mlp.state_dict())\n",
    "        #print(\"lstm weights\", self.lstm.state_dict())\n",
    "      \n",
    "    def forward(self, x):\n",
    "        # get warped points\n",
    "        t_lstm = time.time()\n",
    "        p_canonical, warping_param, intermediate_xyzs = self.lstm(x)\n",
    "\n",
    "        if self.training:\n",
    "            # get sdf value for each intermediate warping step\n",
    "            sdf_values = []\n",
    "            for points in intermediate_xyzs:\n",
    "                sdf = self.mlp(points)\n",
    "                sdf_values.append(sdf.squeeze())\n",
    "            return sdf_values, intermediate_xyzs\n",
    "        else:\n",
    "            # only final sdf value\n",
    "            print(\"no_train\")\n",
    "            sdf_value = self.mlp(p_canonical)\n",
    "            print(\"sdf_value\",sdf_value)\n",
    "            return sdf_value, p_canonical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKW4uYhSSgJn"
   },
   "source": [
    "## Loss Function\n",
    "### Regularization Loss, Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Fq2dbhxJ0jx2"
   },
   "outputs": [],
   "source": [
    "# Loss Function code\n",
    "# DeepSDF: https://github.com/facebookresearch/DeepSDF\n",
    "# Deep Implicit Templates: https://github.com/ZhengZerong/DeepImplicitTemplates\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#                         Reconstruction loss\n",
    "#----------------------------------------------------------------------\n",
    "def reconstruction_loss(sdf_value_list, ground_truth):\n",
    "    #print(\"reconstruction_loss\")\n",
    "\n",
    "    eps_list = [0.025, 0.01, 0.0025, 0]\n",
    "    lambda_list = [0, 0.1, 0.2, 0.5]\n",
    "    loss = []\n",
    "    for s in range(len(eps_list)): # for each warping step \n",
    "        # loss from curriculum DeepSDF\n",
    "        # https://arxiv.org/pdf/2003.08593.pdf\n",
    "        dist = ground_truth - sdf_value_list[s]\n",
    "        loss_e = torch.relu(torch.abs(dist) - eps_list[s])\n",
    "        curriculum_loss = (1 + lambda_list[s] * torch.sign(ground_truth) * torch.sign(dist)) * loss_e\n",
    "        loss.append(torch.mean(curriculum_loss))  #TODO sum??\n",
    "        \n",
    "    return sum(loss) / len(loss) # average loss across all the warping steps\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "#                         Regularization loss\n",
    "#----------------------------------------------------------------------\n",
    "def huber_kernel(a, delta):\n",
    "    # https://en.wikipedia.org/wiki/Huber_loss\n",
    "    abs = torch.abs(a)\n",
    "    return torch.where(abs <= delta, (a*a) / 2, delta * (abs - delta/2))\n",
    "\n",
    "def regularization_loss(original_points, warped_points, latent_codes, epoch):\n",
    "    pw_weight = 0.005 ### from git (sofa)\n",
    "    pp_weight = 0.0001 ### from git (sofa)\n",
    "    epsylon = 0.5 ### From paper\n",
    "    code_lambda = 1e-4 # DeepSDF\n",
    "    num_of_samples = len(warped_points)\n",
    "    start = time.time()\n",
    "    # --------------------------\n",
    "    # point wise regularization\n",
    "    # --------------------------\n",
    "    \n",
    "    # distances of original and warped points\n",
    "    delta_p = warped_points - original_points\n",
    "\n",
    "    # normalize with L2-norm\n",
    "    distances_normalized = torch.linalg.norm(delta_p, dim=1, ord=2)\n",
    "\n",
    "    # apply huber kernel and sum up for all samples\n",
    "    pw_loss = torch.sum(huber_kernel(distances_normalized, delta = 0.25)) / num_of_samples #TODO remove / num_of_samples ?\n",
    "   # print(\"Reg Loss: \", time.time() - start)\n",
    "\n",
    "    # --------------------------\n",
    "    # point wise regularization\n",
    "    # --------------------------\n",
    "    # from git!\n",
    "    \n",
    "    # compare point k to all the other points?\n",
    "    k = num_of_samples//8 #from DIT implementation on git\n",
    "    start = time.time()\n",
    "    # || delta_p_i - delta_p_j ||2\n",
    "    distances_diff = delta_p[:k].view(-1,1,3) - delta_p[k:].view(1,-1,3)\n",
    "    norm_delta_p = torch.linalg.norm(distances_diff, dim=2, ord=2)\n",
    "    # || p_i - p_j ||2\n",
    "    start = time.time()\n",
    "    point_diff = original_points[:k].view(-1,1,3) - original_points[k:].view(1,-1,3)\n",
    "    norm_p = torch.linalg.norm(point_diff, dim=2, ord=2) # + 1e-3 ? to not divide by 0\n",
    "    \n",
    "    start = time.time()\n",
    "    # only positive values\n",
    "    pp_loss = torch.sum(torch.nn.functional.relu(norm_delta_p/norm_p - epsylon).flatten()) / num_of_samples #TODO remove / num_of_samples ?\n",
    "    \n",
    "    # --------------------------\n",
    "    # latent code magnitude loss\n",
    "    # --------------------------\n",
    "    start = time.time()\n",
    "    \n",
    "    # from DeepSDF\n",
    "    code_norm = torch.sum(torch.norm(latent_codes, dim=1)) / num_of_samples\n",
    "    latent_code_loss = code_lambda* min(1.0, epoch/100) * code_norm.cuda()   ### TODO 1/std^2 sum over k ||ck||2^2\n",
    "    return pw_weight * pw_loss + pp_weight * pp_loss + latent_code_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "R5vfxvD20tqS"
   },
   "outputs": [],
   "source": [
    "log_dir = \"\"\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "id": "0sJts7m-_3Kd",
    "outputId": "a3452546-92a5-4aa2-9853-97832ac428ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0005\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "Epoch 0  ended after : 148.74156737327576 seconds with epoch loss: 12.280261078441981\n",
      "Epoch 1  ended after : 147.39031457901 seconds with epoch loss: 1.4838129887357354\n",
      "Epoch 2  ended after : 148.27231550216675 seconds with epoch loss: 1.259433196275495\n",
      "Epoch 3  ended after : 149.38356828689575 seconds with epoch loss: 1.1477044753846712\n",
      "Epoch 4  ended after : 150.47797775268555 seconds with epoch loss: 1.1428906040964648\n",
      "Epoch 5  ended after : 148.46065497398376 seconds with epoch loss: 1.0542296629282646\n",
      "Epoch 6  ended after : 147.29500269889832 seconds with epoch loss: 1.0261656538932584\n",
      "Epoch 7  ended after : 147.5765597820282 seconds with epoch loss: 1.0263275767792948\n",
      "Epoch 8  ended after : 148.5672676563263 seconds with epoch loss: 1.0073932200612035\n",
      "Epoch 9  ended after : 145.6007878780365 seconds with epoch loss: 0.9857839144824538\n",
      "Epoch 10  ended after : 146.00203919410706 seconds with epoch loss: 0.9618878625915386\n",
      "Epoch 11  ended after : 146.86188554763794 seconds with epoch loss: 0.9646292764518876\n",
      "Epoch 12  ended after : 150.66098713874817 seconds with epoch loss: 0.945551887358306\n",
      "Epoch 13  ended after : 149.40121173858643 seconds with epoch loss: 0.9320702145341784\n",
      "Epoch 14  ended after : 156.48644876480103 seconds with epoch loss: 0.9331278494792059\n",
      "Epoch 15  ended after : 195.2776439189911 seconds with epoch loss: 0.9267830185126513\n",
      "Epoch 16  ended after : 193.4865972995758 seconds with epoch loss: 0.9883607046213001\n",
      "Epoch 17  ended after : 197.5260033607483 seconds with epoch loss: 0.9162922425603028\n",
      "Epoch 18  ended after : 193.07665872573853 seconds with epoch loss: 0.9091060388309415\n",
      "Epoch 19  ended after : 199.15931582450867 seconds with epoch loss: 0.9468756548012607\n",
      "Epoch 20  ended after : 194.3730080127716 seconds with epoch loss: 0.9096418448316399\n",
      "Epoch 21  ended after : 193.5112750530243 seconds with epoch loss: 0.9090864044846967\n",
      "Epoch 22  ended after : 201.6276888847351 seconds with epoch loss: 0.8894527195661794\n",
      "Epoch 23  ended after : 195.92827653884888 seconds with epoch loss: 0.8964817384548951\n",
      "Epoch 24  ended after : 194.8313307762146 seconds with epoch loss: 0.8957077280210797\n",
      "Epoch 25  ended after : 207.54668855667114 seconds with epoch loss: 0.8905441085516941\n",
      "Epoch 26  ended after : 212.11877417564392 seconds with epoch loss: 0.8869940558215603\n",
      "Epoch 27  ended after : 194.6392765045166 seconds with epoch loss: 0.8928870484232903\n",
      "Epoch 28  ended after : 194.64753794670105 seconds with epoch loss: 0.8866891368525103\n",
      "Epoch 29  ended after : 197.92586278915405 seconds with epoch loss: 0.8802361840789672\n",
      "Epoch 30  ended after : 194.75843286514282 seconds with epoch loss: 0.8812043724756222\n",
      "Epoch 31  ended after : 211.8534746170044 seconds with epoch loss: 0.8822988493775483\n",
      "Epoch 32  ended after : 224.07287502288818 seconds with epoch loss: 0.8814774302882142\n",
      "Epoch 33  ended after : 198.15542721748352 seconds with epoch loss: 0.8778362397570163\n",
      "Epoch 34  ended after : 195.965247631073 seconds with epoch loss: 0.8794389601389412\n",
      "Epoch 35  ended after : 195.0074906349182 seconds with epoch loss: 0.8731799243250862\n",
      "Epoch 36  ended after : 201.6438536643982 seconds with epoch loss: 0.8664116728759836\n",
      "Epoch 37  ended after : 207.4080798625946 seconds with epoch loss: 0.8687594102812\n",
      "Epoch 38  ended after : 197.51721692085266 seconds with epoch loss: 0.870887642231537\n",
      "Epoch 39  ended after : 191.74381375312805 seconds with epoch loss: 0.868567465164233\n",
      "Epoch 40  ended after : 194.36717462539673 seconds with epoch loss: 0.8669189591309987\n",
      "Epoch 41  ended after : 193.04613995552063 seconds with epoch loss: 0.8570481037313584\n",
      "Epoch 42  ended after : 196.4024040699005 seconds with epoch loss: 0.8650406317028683\n",
      "Epoch 43  ended after : 195.6334593296051 seconds with epoch loss: 0.8599403517728206\n",
      "Epoch 44  ended after : 196.7691366672516 seconds with epoch loss: 0.8636458714900073\n",
      "Epoch 45  ended after : 189.86661028862 seconds with epoch loss: 0.8552584789867979\n",
      "Epoch 46  ended after : 194.09875774383545 seconds with epoch loss: 0.8617401562805753\n",
      "Epoch 47  ended after : 196.06226658821106 seconds with epoch loss: 0.8604123019613326\n",
      "Epoch 48  ended after : 193.29014706611633 seconds with epoch loss: 0.8575891302898526\n",
      "Epoch 49  ended after : 192.1844654083252 seconds with epoch loss: 0.8568302549829241\n",
      "Epoch 50  ended after : 194.17319631576538 seconds with epoch loss: 0.8527131178125273\n",
      "Epoch 51  ended after : 195.86276721954346 seconds with epoch loss: 0.8470532798091881\n",
      "Epoch 52  ended after : 193.2343590259552 seconds with epoch loss: 0.8462292507756501\n",
      "Epoch 53  ended after : 194.4747326374054 seconds with epoch loss: 0.8420777391002048\n",
      "Epoch 54  ended after : 196.82532286643982 seconds with epoch loss: 0.8458503415458836\n",
      "Epoch 55  ended after : 192.39262557029724 seconds with epoch loss: 0.8428253278543707\n",
      "Epoch 56  ended after : 196.47765564918518 seconds with epoch loss: 0.8451535016938578\n",
      "Epoch 57  ended after : 192.3474760055542 seconds with epoch loss: 0.8411323960172012\n",
      "Epoch 58  ended after : 197.73632431030273 seconds with epoch loss: 0.8352870692615397\n",
      "Epoch 59  ended after : 190.1076307296753 seconds with epoch loss: 0.8412173931428697\n",
      "Epoch 60  ended after : 194.83773732185364 seconds with epoch loss: 0.8287839432014152\n",
      "Epoch 61  ended after : 195.80796217918396 seconds with epoch loss: 0.8317928637843579\n",
      "Epoch 62  ended after : 197.0872871875763 seconds with epoch loss: 0.8317146270128433\n",
      "Epoch 63  ended after : 195.26419973373413 seconds with epoch loss: 0.8243970504845493\n",
      "Epoch 64  ended after : 197.04656457901 seconds with epoch loss: 0.8227256629616022\n",
      "Epoch 65  ended after : 196.26069402694702 seconds with epoch loss: 0.8222117553814314\n",
      "Epoch 66  ended after : 193.9944257736206 seconds with epoch loss: 0.8197607356705703\n",
      "Epoch 67  ended after : 196.7892427444458 seconds with epoch loss: 0.8237542336282786\n",
      "Epoch 68  ended after : 194.61078691482544 seconds with epoch loss: 0.817248394159833\n",
      "Epoch 69  ended after : 197.3405785560608 seconds with epoch loss: 0.8114684321335517\n",
      "Epoch 70  ended after : 194.396386384964 seconds with epoch loss: 0.811625539790839\n",
      "Epoch 71  ended after : 197.78420495986938 seconds with epoch loss: 0.8121977691771463\n",
      "Epoch 72  ended after : 192.3822727203369 seconds with epoch loss: 0.8079651395091787\n",
      "Epoch 73  ended after : 197.34733533859253 seconds with epoch loss: 0.8118252909916919\n",
      "Epoch 74  ended after : 197.91240692138672 seconds with epoch loss: 0.8021867669303901\n",
      "Epoch 75  ended after : 193.84944200515747 seconds with epoch loss: 0.8063737079210114\n",
      "Epoch 76  ended after : 196.7206962108612 seconds with epoch loss: 0.7991256218811031\n",
      "Epoch 77  ended after : 195.2157084941864 seconds with epoch loss: 0.7982209578622133\n",
      "Epoch 78  ended after : 172.57449054718018 seconds with epoch loss: 0.8023512459476478\n",
      "Epoch 79  ended after : 149.47664141654968 seconds with epoch loss: 0.7947547747462522\n",
      "Epoch 80  ended after : 148.22822999954224 seconds with epoch loss: 0.7953706683183555\n",
      "Epoch 81  ended after : 146.80164122581482 seconds with epoch loss: 0.7899721012508962\n",
      "Epoch 82  ended after : 148.68965315818787 seconds with epoch loss: 0.7983170548395719\n",
      "Epoch 83  ended after : 150.92457509040833 seconds with epoch loss: 0.780630634049885\n",
      "Epoch 84  ended after : 169.06536602973938 seconds with epoch loss: 0.7906730339454953\n",
      "Epoch 85  ended after : 149.50670886039734 seconds with epoch loss: 0.789620685565751\n",
      "Epoch 86  ended after : 155.45396375656128 seconds with epoch loss: 0.7880213383177761\n",
      "Epoch 87  ended after : 155.28493642807007 seconds with epoch loss: 0.7850751593359746\n",
      "Epoch 88  ended after : 155.37666726112366 seconds with epoch loss: 0.7850898802280426\n",
      "Epoch 89  ended after : 155.13779497146606 seconds with epoch loss: 0.7838839280302636\n",
      "Epoch 90  ended after : 156.22759699821472 seconds with epoch loss: 0.7787677045853343\n",
      "Epoch 91  ended after : 154.9407923221588 seconds with epoch loss: 0.7890940006182063\n",
      "Epoch 92  ended after : 156.69485688209534 seconds with epoch loss: 0.7805299598840065\n",
      "Epoch 93  ended after : 154.91813778877258 seconds with epoch loss: 0.7761516914470121\n",
      "Epoch 94  ended after : 162.9895055294037 seconds with epoch loss: 0.7753744917281438\n",
      "Epoch 95  ended after : 156.31788754463196 seconds with epoch loss: 0.7756664187763818\n",
      "Epoch 96  ended after : 157.32707476615906 seconds with epoch loss: 0.7804409432283137\n",
      "Epoch 97  ended after : 156.41823530197144 seconds with epoch loss: 0.7803895187098533\n",
      "Epoch 98  ended after : 155.93991875648499 seconds with epoch loss: 0.7708985902136192\n",
      "Epoch 99  ended after : 155.19482231140137 seconds with epoch loss: 0.7734218411205802\n",
      "Epoch 100  ended after : 158.11860036849976 seconds with epoch loss: 0.763718587171752\n",
      "Epoch 101  ended after : 154.3882462978363 seconds with epoch loss: 0.7700126850686502\n",
      "Epoch 102  ended after : 155.43089938163757 seconds with epoch loss: 0.7651482637447771\n",
      "Epoch 103  ended after : 156.91052651405334 seconds with epoch loss: 0.7784876695659477\n",
      "Epoch 104  ended after : 155.5404257774353 seconds with epoch loss: 0.7699136379233096\n",
      "Epoch 105  ended after : 154.83111357688904 seconds with epoch loss: 0.7629252320912201\n",
      "Epoch 106  ended after : 159.25787162780762 seconds with epoch loss: 0.7646064019936603\n",
      "Epoch 107  ended after : 156.70693016052246 seconds with epoch loss: 0.7693110922991764\n",
      "Epoch 108  ended after : 155.09389877319336 seconds with epoch loss: 0.7644637206685729\n",
      "Epoch 109  ended after : 155.87782764434814 seconds with epoch loss: 0.7669967372494284\n",
      "Epoch 110  ended after : 157.07085728645325 seconds with epoch loss: 0.7620281744457316\n",
      "Epoch 111  ended after : 155.11897015571594 seconds with epoch loss: 0.7596774340781849\n",
      "Epoch 112  ended after : 156.61074233055115 seconds with epoch loss: 0.7552175107121002\n",
      "Epoch 113  ended after : 156.86009812355042 seconds with epoch loss: 0.7568942029902246\n",
      "Epoch 114  ended after : 157.52411031723022 seconds with epoch loss: 0.76371248945361\n",
      "Epoch 115  ended after : 156.33195161819458 seconds with epoch loss: 0.7570144600758795\n",
      "Epoch 116  ended after : 154.95765113830566 seconds with epoch loss: 0.7598024441685993\n",
      "Epoch 117  ended after : 156.06562685966492 seconds with epoch loss: 0.7588692801946308\n",
      "Epoch 118  ended after : 154.34856748580933 seconds with epoch loss: 0.756364950153511\n",
      "Epoch 119  ended after : 156.5273470878601 seconds with epoch loss: 0.7619324234547094\n",
      "Epoch 120  ended after : 154.8785901069641 seconds with epoch loss: 0.7489816014713142\n",
      "Epoch 121  ended after : 155.65545082092285 seconds with epoch loss: 0.7610879027924966\n",
      "Epoch 122  ended after : 156.55379986763 seconds with epoch loss: 0.7582845855795313\n",
      "Epoch 123  ended after : 155.2362687587738 seconds with epoch loss: 0.7587117947987281\n",
      "Epoch 124  ended after : 157.28938579559326 seconds with epoch loss: 0.7508493524510413\n",
      "Epoch 125  ended after : 156.20791268348694 seconds with epoch loss: 0.754874895705143\n",
      "Epoch 126  ended after : 155.80460667610168 seconds with epoch loss: 0.7419432214810513\n",
      "Epoch 127  ended after : 156.48984694480896 seconds with epoch loss: 0.7502591299708001\n",
      "Epoch 128  ended after : 157.06634950637817 seconds with epoch loss: 0.7571936146268854\n",
      "Epoch 129  ended after : 157.42392802238464 seconds with epoch loss: 0.755824329360621\n",
      "Epoch 130  ended after : 157.32254886627197 seconds with epoch loss: 0.7502571661607362\n",
      "Epoch 131  ended after : 156.80875515937805 seconds with epoch loss: 0.7507613085617777\n",
      "Epoch 132  ended after : 155.67874264717102 seconds with epoch loss: 0.744925366161624\n",
      "Epoch 133  ended after : 157.14542508125305 seconds with epoch loss: 0.7471102977142436\n",
      "Epoch 134  ended after : 157.1493957042694 seconds with epoch loss: 0.7528644768171944\n",
      "Epoch 135  ended after : 155.11818289756775 seconds with epoch loss: 0.7477785959199537\n",
      "Epoch 136  ended after : 154.63493990898132 seconds with epoch loss: 0.7513189033488743\n",
      "Epoch 137  ended after : 155.53164672851562 seconds with epoch loss: 0.7473668512539007\n",
      "Epoch 138  ended after : 156.02738237380981 seconds with epoch loss: 0.7404818964423612\n",
      "Epoch 139  ended after : 155.95412921905518 seconds with epoch loss: 0.7314271798823029\n",
      "Epoch 140  ended after : 155.7731249332428 seconds with epoch loss: 0.7378189757582732\n",
      "Epoch 141  ended after : 159.25455832481384 seconds with epoch loss: 0.7328610792610561\n",
      "Epoch 142  ended after : 156.00384163856506 seconds with epoch loss: 0.7277567709243158\n",
      "Epoch 143  ended after : 156.9593803882599 seconds with epoch loss: 0.7339773364947177\n",
      "Epoch 144  ended after : 156.01253604888916 seconds with epoch loss: 0.7225089373096125\n",
      "Epoch 145  ended after : 156.5881404876709 seconds with epoch loss: 0.7182305999740493\n",
      "Epoch 146  ended after : 154.16953134536743 seconds with epoch loss: 0.7252632771269418\n",
      "Epoch 147  ended after : 155.37070178985596 seconds with epoch loss: 0.7267035992699675\n",
      "Epoch 148  ended after : 156.5919964313507 seconds with epoch loss: 0.7143397914769594\n",
      "Epoch 149  ended after : 154.4463324546814 seconds with epoch loss: 0.7217028967716033\n",
      "Epoch 150  ended after : 155.387708902359 seconds with epoch loss: 0.7158301812887657\n",
      "Epoch 151  ended after : 157.03381204605103 seconds with epoch loss: 0.7137827758997446\n",
      "Epoch 152  ended after : 155.18582272529602 seconds with epoch loss: 0.7281477752985666\n",
      "Epoch 153  ended after : 158.67817330360413 seconds with epoch loss: 0.7201455002941657\n",
      "Epoch 154  ended after : 158.08037400245667 seconds with epoch loss: 0.713040619622916\n",
      "Epoch 155  ended after : 155.74223351478577 seconds with epoch loss: 0.7182371862436412\n",
      "Epoch 156  ended after : 155.0810718536377 seconds with epoch loss: 0.7121452303254046\n",
      "Epoch 157  ended after : 153.534903049469 seconds with epoch loss: 0.7148183141252957\n",
      "Epoch 158  ended after : 156.4151635169983 seconds with epoch loss: 0.7097521420655539\n",
      "Epoch 159  ended after : 154.8818542957306 seconds with epoch loss: 0.698787082452327\n",
      "Epoch 160  ended after : 154.916827917099 seconds with epoch loss: 0.714291101161507\n",
      "Epoch 161  ended after : 155.78659892082214 seconds with epoch loss: 0.7135383312852355\n",
      "Epoch 162  ended after : 155.12042832374573 seconds with epoch loss: 0.7119636052520946\n",
      "Epoch 163  ended after : 155.93454146385193 seconds with epoch loss: 0.700960368689266\n",
      "Epoch 164  ended after : 155.56683135032654 seconds with epoch loss: 0.7043774200283224\n",
      "Epoch 165  ended after : 154.58136868476868 seconds with epoch loss: 0.7084173592738807\n",
      "Epoch 166  ended after : 157.70375442504883 seconds with epoch loss: 0.7012219962780364\n",
      "Epoch 167  ended after : 155.6510202884674 seconds with epoch loss: 0.7053918552701361\n",
      "Epoch 168  ended after : 155.59315276145935 seconds with epoch loss: 0.7081728850898799\n",
      "Epoch 169  ended after : 157.16885948181152 seconds with epoch loss: 0.7014090435404796\n",
      "Epoch 170  ended after : 155.9668731689453 seconds with epoch loss: 0.6977356984134531\n",
      "Epoch 171  ended after : 155.2415065765381 seconds with epoch loss: 0.6970521930779796\n",
      "Epoch 172  ended after : 156.77501940727234 seconds with epoch loss: 0.7025521262985421\n",
      "Epoch 173  ended after : 154.79488897323608 seconds with epoch loss: 0.7102410104562296\n",
      "Epoch 174  ended after : 155.45456838607788 seconds with epoch loss: 0.7018475954973837\n",
      "Epoch 175  ended after : 155.10311770439148 seconds with epoch loss: 0.69939007134235\n",
      "Epoch 176  ended after : 156.0399191379547 seconds with epoch loss: 0.7041344234603457\n",
      "Epoch 177  ended after : 159.98463201522827 seconds with epoch loss: 0.702201911190059\n",
      "Epoch 178  ended after : 155.72951769828796 seconds with epoch loss: 0.6975310935231391\n",
      "Epoch 179  ended after : 156.47889018058777 seconds with epoch loss: 0.7044071557902498\n",
      "Epoch 180  ended after : 156.43133354187012 seconds with epoch loss: 0.6957229948457098\n",
      "Epoch 181  ended after : 155.45737528800964 seconds with epoch loss: 0.7029596735228552\n",
      "Epoch 182  ended after : 157.61776995658875 seconds with epoch loss: 0.7047177708591335\n",
      "Epoch 183  ended after : 154.88194680213928 seconds with epoch loss: 0.6999786614323966\n",
      "Epoch 184  ended after : 154.73575472831726 seconds with epoch loss: 0.6992010701505933\n",
      "Epoch 185  ended after : 155.25813555717468 seconds with epoch loss: 0.697263233945705\n",
      "Epoch 186  ended after : 155.1575255393982 seconds with epoch loss: 0.6977016432938399\n",
      "Epoch 187  ended after : 155.60186433792114 seconds with epoch loss: 0.6955288170283893\n",
      "Epoch 188  ended after : 156.24971723556519 seconds with epoch loss: 0.6977458331821254\n",
      "Epoch 189  ended after : 155.11318016052246 seconds with epoch loss: 0.6853488686610945\n",
      "Epoch 190  ended after : 156.12304878234863 seconds with epoch loss: 0.6903173058235552\n",
      "Epoch 191  ended after : 155.12947797775269 seconds with epoch loss: 0.7023892138531664\n",
      "Epoch 192  ended after : 155.2526307106018 seconds with epoch loss: 0.6977807872171979\n",
      "Epoch 193  ended after : 155.1459641456604 seconds with epoch loss: 0.6946354846440954\n",
      "Epoch 194  ended after : 156.91290593147278 seconds with epoch loss: 0.693044986910536\n",
      "Epoch 195  ended after : 155.7622447013855 seconds with epoch loss: 0.7030707600642927\n",
      "Epoch 196  ended after : 154.91543173789978 seconds with epoch loss: 0.698317427362781\n",
      "Epoch 197  ended after : 154.32571506500244 seconds with epoch loss: 0.6985735419439152\n",
      "Epoch 198  ended after : 155.2429540157318 seconds with epoch loss: 0.6939497881830903\n",
      "Epoch 199  ended after : 157.0205569267273 seconds with epoch loss: 0.6870697823469527\n",
      "Epoch 200  ended after : 157.39958596229553 seconds with epoch loss: 0.6936917953717057\n",
      "Epoch 201  ended after : 155.3489408493042 seconds with epoch loss: 0.6943385108024813\n",
      "Epoch 202  ended after : 156.4385576248169 seconds with epoch loss: 0.7250283678586129\n",
      "Epoch 203  ended after : 157.72382974624634 seconds with epoch loss: 0.6892542012938065\n",
      "Epoch 204  ended after : 154.80497217178345 seconds with epoch loss: 0.6891262401768472\n",
      "Epoch 205  ended after : 156.18486547470093 seconds with epoch loss: 0.6936709271103609\n",
      "Epoch 206  ended after : 155.3292510509491 seconds with epoch loss: 0.6877761518844636\n",
      "Epoch 207  ended after : 155.9168577194214 seconds with epoch loss: 0.6882478328916477\n",
      "Epoch 208  ended after : 155.61317944526672 seconds with epoch loss: 0.6897718627005816\n",
      "Epoch 209  ended after : 154.9808225631714 seconds with epoch loss: 0.6886159876739839\n",
      "Epoch 210  ended after : 155.91811800003052 seconds with epoch loss: 0.6870345806528348\n",
      "Epoch 211  ended after : 157.20520091056824 seconds with epoch loss: 0.6840815912000835\n",
      "Epoch 212  ended after : 154.96816301345825 seconds with epoch loss: 0.693190739635611\n",
      "Epoch 213  ended after : 155.99248552322388 seconds with epoch loss: 0.6922075197799131\n",
      "Epoch 214  ended after : 155.62744522094727 seconds with epoch loss: 0.6901136895030504\n",
      "Epoch 215  ended after : 156.7518928050995 seconds with epoch loss: 0.6873826624505455\n",
      "Epoch 216  ended after : 155.4684112071991 seconds with epoch loss: 0.6841738834482385\n",
      "Epoch 217  ended after : 156.9643337726593 seconds with epoch loss: 0.6862496342509985\n",
      "Epoch 218  ended after : 160.23556661605835 seconds with epoch loss: 0.697990772765479\n",
      "Epoch 219  ended after : 154.52220630645752 seconds with epoch loss: 0.6882994045590749\n",
      "Epoch 220  ended after : 156.04919266700745 seconds with epoch loss: 0.6876372757105855\n",
      "Epoch 221  ended after : 154.20269417762756 seconds with epoch loss: 0.6900265206350014\n",
      "Epoch 222  ended after : 156.4625346660614 seconds with epoch loss: 0.6851610090234317\n",
      "Epoch 223  ended after : 156.99527382850647 seconds with epoch loss: 0.6831133637315361\n",
      "Epoch 224  ended after : 156.27643752098083 seconds with epoch loss: 0.6818995178909972\n",
      "Epoch 225  ended after : 153.94146370887756 seconds with epoch loss: 0.6782205375639023\n",
      "Epoch 226  ended after : 156.26394987106323 seconds with epoch loss: 0.6845152817986673\n",
      "Epoch 227  ended after : 155.8537154197693 seconds with epoch loss: 0.6827165493596112\n",
      "Epoch 228  ended after : 155.69331049919128 seconds with epoch loss: 0.6846248584479326\n",
      "Epoch 229  ended after : 154.68928003311157 seconds with epoch loss: 0.6801979756128276\n",
      "Epoch 230  ended after : 156.53562259674072 seconds with epoch loss: 0.6902566460194066\n",
      "Epoch 231  ended after : 154.91235756874084 seconds with epoch loss: 0.6829551497066859\n",
      "Epoch 232  ended after : 154.54607319831848 seconds with epoch loss: 0.6752713092573686\n",
      "Epoch 233  ended after : 156.1730091571808 seconds with epoch loss: 0.6827169930038508\n",
      "Epoch 234  ended after : 155.68437433242798 seconds with epoch loss: 0.6782781155052362\n",
      "Epoch 235  ended after : 155.89895486831665 seconds with epoch loss: 0.6816410027968232\n",
      "Epoch 236  ended after : 154.8350465297699 seconds with epoch loss: 0.6769126934814267\n",
      "Epoch 237  ended after : 155.17542719841003 seconds with epoch loss: 0.6767586027999641\n",
      "Epoch 238  ended after : 155.26852822303772 seconds with epoch loss: 0.6819536582188448\n",
      "Epoch 239  ended after : 156.14890789985657 seconds with epoch loss: 0.6725986023520818\n",
      "Epoch 240  ended after : 157.12741684913635 seconds with epoch loss: 0.675386769886245\n",
      "Epoch 241  ended after : 155.02968001365662 seconds with epoch loss: 0.6849790256674169\n",
      "Epoch 242  ended after : 155.47238898277283 seconds with epoch loss: 0.6780033120448934\n",
      "Epoch 243  ended after : 155.26650166511536 seconds with epoch loss: 0.6753348249185365\n",
      "Epoch 244  ended after : 155.8330934047699 seconds with epoch loss: 0.6692865169607103\n",
      "Epoch 245  ended after : 155.6126992702484 seconds with epoch loss: 0.6764185022766469\n",
      "Epoch 246  ended after : 154.44608354568481 seconds with epoch loss: 0.67694596957881\n",
      "Epoch 247  ended after : 155.63114666938782 seconds with epoch loss: 0.6782085962622659\n",
      "Epoch 248  ended after : 154.5117540359497 seconds with epoch loss: 0.6642604124645004\n",
      "Epoch 249  ended after : 156.57515406608582 seconds with epoch loss: 0.6720802377385553\n",
      "Epoch 250  ended after : 155.68430376052856 seconds with epoch loss: 0.6765025047207018\n",
      "Epoch 251  ended after : 155.02750205993652 seconds with epoch loss: 0.6725453996623401\n",
      "Epoch 252  ended after : 156.2811963558197 seconds with epoch loss: 0.6749200207996182\n",
      "Epoch 253  ended after : 155.34326362609863 seconds with epoch loss: 0.6679809759079944\n",
      "Epoch 254  ended after : 154.81186938285828 seconds with epoch loss: 0.6699415983166546\n",
      "Epoch 255  ended after : 155.4526607990265 seconds with epoch loss: 0.6748094836657401\n",
      "Epoch 256  ended after : 155.02589583396912 seconds with epoch loss: 0.6731176165048964\n",
      "Epoch 257  ended after : 156.11576986312866 seconds with epoch loss: 0.6715665608790005\n",
      "Epoch 258  ended after : 155.22209572792053 seconds with epoch loss: 0.6693964366131695\n",
      "Epoch 259  ended after : 151.93294429779053 seconds with epoch loss: 0.6769472720334306\n",
      "Epoch 260  ended after : 148.72356057167053 seconds with epoch loss: 0.6656085691502085\n",
      "Epoch 261  ended after : 149.27712154388428 seconds with epoch loss: 0.6707458320161095\n",
      "Epoch 262  ended after : 148.859148979187 seconds with epoch loss: 0.671951835625805\n",
      "Epoch 263  ended after : 148.92023015022278 seconds with epoch loss: 0.6676138266047928\n",
      "Epoch 264  ended after : 149.09841346740723 seconds with epoch loss: 0.669820577284554\n",
      "Epoch 265  ended after : 148.66592454910278 seconds with epoch loss: 0.6638057342352113\n",
      "Epoch 266  ended after : 148.99242329597473 seconds with epoch loss: 0.6791619597061072\n",
      "Epoch 267  ended after : 148.82723236083984 seconds with epoch loss: 0.6716481095791096\n",
      "Epoch 268  ended after : 148.67443084716797 seconds with epoch loss: 0.6649588255822891\n",
      "Epoch 269  ended after : 149.2409381866455 seconds with epoch loss: 0.6700663032825105\n",
      "Epoch 270  ended after : 149.45161986351013 seconds with epoch loss: 0.6743426390894456\n",
      "Epoch 271  ended after : 148.48097825050354 seconds with epoch loss: 0.6689882342325291\n"
     ]
    }
   ],
   "source": [
    "# initialize Network\n",
    "from easydict.easydict import EasyDict as edict\n",
    "#from easydict import EasyDict as edict\n",
    "\n",
    "lstm_args = edict()\n",
    "lstm_args.steps = 8\n",
    "lstm_args.latent_size = 256\n",
    "lstm_args.h_size = 512\n",
    "lstm_args.dim = [512, 64]\n",
    "\n",
    "lat_vecs = torch.nn.Embedding(len(sdf_loader), lstm_args.latent_size, max_norm=1)\n",
    "#torch.nn.init.normal_(lat_vecs.weight.data, 0.0, 1.0/math.sqrt(lat_vecs))\n",
    "\n",
    "mlp_args = edict()\n",
    "mlp_args.layers = [256,256,256,256,256]\n",
    "mlp_args.wn_layers = [0,1,2,3,4]\n",
    "mlp_args.weight_norm = True\n",
    "mlp_args.dropout_layers =[0,1,2,3,4]\n",
    "mlp_args.dropout_prob = 0.05\n",
    "\n",
    "decoder = DIT(mlp_args, lstm_args)\n",
    "decoder = decoder.to(device) # use GPU\n",
    "\n",
    "\n",
    "# Training\n",
    "\n",
    "#learning_rate = 0.001 #? # 0.01 was not helping (the epoch loss was not decreasing)\n",
    "#print(decoder.parameters())\n",
    "optimizer = optim.Adam([\n",
    "    {\"params\":decoder.parameters(), \"lr\":0.0005,},\n",
    "    {\"params\":lat_vecs.parameters(), \"lr\":0.001,}, ] )\n",
    "\n",
    "print(\"optimizer\",optimizer)\n",
    "# TODO shuffle the data somehow\n",
    "# TODO split data into training and test data\n",
    "\n",
    "\n",
    "mini_batch_size =5000 \n",
    "num_epochs = 2000 #2001\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    epoch_loss = 0\n",
    "    decoder.train()\n",
    " #   print(\"lat_vecs\",lat_vecs(torch.IntTensor([0])))\n",
    "    for a, (shape_data, latent_code_index) in enumerate(sdf_loader): # for each shape\n",
    "        shape_start_time = time.time()\n",
    "        # loose first dimension (shape dimension)\n",
    "        data = shape_data.data.reshape(-1, 4)\n",
    "       # print(\"data.shape\", shape_data.shape)\n",
    "\n",
    "        num_samples = data.shape[0]\n",
    "       # print(\"number of samples\", num_samples)\n",
    "\n",
    "        # format data for the whole shape\n",
    "        xyz = data[:,0:3] # point coordinates\n",
    "        xyz.requires_grad = False\n",
    "        sdf_gt = data[:,3] # ground truth sdf values\n",
    "        sdf_gt.requires_grad = False\n",
    "        sdf_gt = torch.clamp(sdf_gt, -0.1, 0.1)\n",
    "        \n",
    "        lat_indices = latent_code_index.repeat(num_samples) # repeated latent code index\n",
    "\n",
    "        shape_loss = 0\n",
    "        \n",
    "        num_batches = num_samples // mini_batch_size\n",
    "        for i in range(num_batches):\n",
    "            start_batch_time = time.time()\n",
    "            #print('batches number:', i)\n",
    "            # for each batch\n",
    "            start_index = i * mini_batch_size\n",
    "            end_index = start_index + mini_batch_size\n",
    "            if(end_index > num_samples):\n",
    "                end_index = num_samples\n",
    "\n",
    "            # get batch data\n",
    "            batch_xyz = xyz[start_index: end_index].to(device) # point coordinates\n",
    "            batch_sdf_gt = sdf_gt[start_index: end_index].to(device) # ground truth values\n",
    "            batch_lat_indices = lat_indices[start_index: end_index]\n",
    "            batch_lat_codes = lat_vecs(batch_lat_indices).to(device) # latent codes\n",
    "            input = torch.cat([batch_lat_codes, batch_xyz], dim=1)\n",
    "            \n",
    "            #print(\"input before\",input)\n",
    "            #print(\"batch_sdf_gt\",batch_sdf_gt)\n",
    "            # feed input into network\n",
    "            sdf_values, warped_points = decoder(input)\n",
    "            for i in range(len(sdf_values)):\n",
    "                sdf_values[i] = torch.clamp(sdf_values[i], -0.1, 0.1)\n",
    "            #print(\"sdf_values\",sdf_values)\n",
    "            # compute loss\n",
    "            start_loss_time = time.time()\n",
    "            rec_loss = reconstruction_loss(sdf_values, batch_sdf_gt)\n",
    "            \n",
    "            end_rec_time = time.time()\n",
    "            \n",
    "            reg_loss = regularization_loss(batch_xyz, warped_points[-1], batch_lat_codes, epoch) # final_positions\n",
    "            \n",
    "            end_loss_time = time.time()\n",
    "            \n",
    "         #   print(\"REC LOSS TIME:\",end_rec_time-start_loss_time)\n",
    "         #   print(\"REG LOSS TIME:\",end_loss_time-end_rec_time)\n",
    "\n",
    "            #print(\"REC LOSS:\",rec_loss.item(),\"REG LOSS:\",reg_loss.item())#,\"min sdf: \",torch.min(sdf_values[1]))\n",
    "            #print(\"REC LOSS:\",rec_loss,\"REG LOSS:\",reg_loss)\n",
    "            batch_loss = rec_loss + reg_loss # / (end_index - start_index)\n",
    "            shape_loss += batch_loss.item() \n",
    "            # / (end_index - start_index)\n",
    "\n",
    "            start_optim_time = time.time()\n",
    "            # flush out previously computed gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backpropagation\n",
    "            batch_loss.backward()\n",
    "            \n",
    "#             print(\"input middle\",input)\n",
    "#             print(\"-------\")\n",
    "#             for name, param in decoder.named_parameters():\n",
    "#                 print(name, param.grad)\n",
    "\n",
    "            # update network weights\n",
    "            optimizer.step()\n",
    "            \n",
    "     #       print(\"input after\",input)\n",
    "            end_optim_time = time.time()\n",
    "          #  print(\"OPTIM TIME:\", end_optim_time-start_optim_time)\n",
    "\n",
    "            \n",
    "            end_batch_time = time.time()\n",
    "       #     print(\"BATCH TIME:\",end_batch_time-start_batch_time)\n",
    "            #print(\"after batch\", i)\n",
    "            # release memory\n",
    "            del sdf_values, warped_points, input, batch_xyz, batch_sdf_gt, batch_lat_indices, batch_lat_codes, start_index, end_index#, rec_loss, reg_loss,final_positions\n",
    "            torch.cuda.empty_cache()\n",
    "        # update tensorboard\n",
    "        writer.add_scalar(\"train_loss\", shape_loss, a)\n",
    "        \n",
    "        shape_end_time = time.time()\n",
    "#        if  a%100 == 0\n",
    " #           print(\"Loss for SHAPE (batch)\", a, \" is:\", shape_loss, \" computed in:\", shape_end_time-shape_start_time, \" seconds\")\n",
    "        \n",
    "        epoch_loss += shape_loss\n",
    "\n",
    "        # print train loss, accuracy, time\n",
    "        del shape_loss, data, xyz, sdf_gt, lat_indices\n",
    "        \n",
    "    # save weights in weights_file\n",
    "    # where to save model weights\n",
    "    weights_file = './weights/weights_%d.pt'\n",
    "    if epoch % 10 == 0:\n",
    "        weights_file_1 = weights_file%epoch\n",
    "        torch.save(decoder.state_dict(),weights_file_1)\n",
    "    \n",
    "    torch.save(decoder.state_dict(),\"./weights/weight_latest.pt\")\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "    print(\"Epoch\", epoch, \" ended after :\", epoch_end_time-epoch_start_time, \"seconds with epoch loss:\", epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepImplicitTemplates.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
